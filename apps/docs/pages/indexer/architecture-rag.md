# Arquitetura RAG do Code Compass Indexer

Este documento explica como funciona o sistema de **Retrieval Augmented Generation (RAG)** implementado no comando `ask` do Indexer.

## VisÃ£o Geral

O RAG combina busca semÃ¢ntica com geraÃ§Ã£o de linguagem natural para responder perguntas sobre o cÃ³digo-fonte. Em vez de o LLM "adivinhar" respostas, ele recebe o contexto relevante e responde baseado nele.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Pergunta   â”‚â”€â”€â”€â”€â–¶â”‚  Embedding  â”‚â”€â”€â”€â”€â–¶â”‚   Qdrant    â”‚
â”‚  (usuÃ¡rio)  â”‚     â”‚  (Ollama)   â”‚     â”‚   (busca)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Resposta   â”‚â—€â”€â”€â”€â”€â”‚     LLM     â”‚â—€â”€â”€â”€â”€â”‚  Contexto   â”‚
â”‚   (texto)   â”‚     â”‚  (Ollama)   â”‚     â”‚  (cÃ³digo)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Etapas do Fluxo RAG

### 1. Embedding da Pergunta

```
Entrada: "como funciona o chunking?"
SaÃ­da:   [0.12, -0.45, 0.78, ..., 0.33]  (vetor 3584-dim)
```

- Usa o mesmo modelo de embedding do indexador (`manutic/nomic-embed-code`)
- Transforma a pergunta em um vetor no mesmo espaÃ§o dos chunks indexados
- Permite comparar semanticamente a pergunta com o cÃ³digo

### 2. Busca no Qdrant (Banco Vetorial)

```
Entrada: vetor da pergunta
SaÃ­da:   5 chunks mais similares (metadados)
         - path: "indexer/chunk.py"
         - start_line: 20
         - end_line: 80
         - score: 0.87
```

**O que o Qdrant armazena:**
- Vetores de embedding (representaÃ§Ã£o numÃ©rica do cÃ³digo)
- Metadados (path, linhas, hash, extensÃ£o, linguagem)

**O que o Qdrant NÃƒO armazena:**
- O texto/cÃ³digo em si (apenas referÃªncias)

**Por que usar busca vetorial?**
- Encontra resultados semanticamente similares, nÃ£o apenas palavras exatas
- "como funciona chunking" encontra cÃ³digo sobre "dividir arquivos em blocos"

### 3. Leitura do CÃ³digo (Filesystem)

```
Entrada: path="indexer/chunk.py", lines=20-80, repo_root="/home/user/project"
SaÃ­da:   conteÃºdo real do arquivo (linhas 20-80)
```

Com os metadados retornados pelo Qdrant, o sistema:
1. Monta o caminho completo: `{repo_root}/{path}`
2. LÃª o arquivo do disco
3. Extrai apenas as linhas relevantes: `lines[start_line:end_line]`

**Por que ler do disco?**
- Garante que o contexto Ã© sempre atualizado
- O Qdrant sÃ³ precisa armazenar referÃªncias (menor uso de memÃ³ria)
- Se o arquivo foi modificado desde a indexaÃ§Ã£o, precisa reindexar

### 4. Montagem do Prompt

O sistema monta um prompt estruturado para o LLM:

```
SYSTEM:
VocÃª Ã© um assistente especializado em analisar cÃ³digo-fonte.
Responda Ã s perguntas do usuÃ¡rio baseando-se APENAS no contexto fornecido.
Se a informaÃ§Ã£o nÃ£o estiver no contexto, diga que nÃ£o encontrou essa informaÃ§Ã£o.
Seja conciso e direto. Responda em portuguÃªs brasileiro.

USER:
## Contexto do cÃ³digo-fonte:

### Arquivo 1: indexer/chunk.py (linhas 20-80)
```python
def chunk_lines(lines: list[str], chunk_size: int, overlap: int) -> list[tuple]:
    """Divide linhas em chunks com overlap."""
    if overlap >= chunk_size:
        raise ValueError("overlap must be < chunk_size")
    ...
```

### Arquivo 2: apps/docs/pages/indexer/commands/chunk.md (linhas 1-50)
```markdown
# Comando Chunk
O comando chunk divide arquivos em pedaÃ§os...
```

## Pergunta:
como funciona o chunking?

## Resposta:
```

### 5. GeraÃ§Ã£o de Resposta (LLM)

```
Entrada: prompt montado com contexto
SaÃ­da:   resposta em linguagem natural
```

O LLM (ex: `qwen3-coder:30b`, `gpt-oss:latest`):
- LÃª o contexto fornecido
- Entende a pergunta
- Sintetiza uma resposta baseada apenas no que viu

**O LLM nÃ£o:**
- Acessa arquivos diretamente
- Busca no Qdrant
- Inventa informaÃ§Ãµes (idealmente)

## Componentes e Responsabilidades

| Componente | Responsabilidade | Armazena |
|------------|------------------|----------|
| **Ollama Embedding** | Converte texto â†’ vetor | - |
| **Qdrant** | Busca vetores similares | Vetores + metadados |
| **Filesystem** | Fornece cÃ³digo real | Arquivos fonte |
| **Ollama LLM** | Gera resposta | - |

## Fluxo de Dados Detalhado

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         INDEXAÃ‡ÃƒO (index)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                    â”‚
â”‚  Arquivos â”€â”€â–¶ Chunks â”€â”€â–¶ Embeddings â”€â”€â–¶ Qdrant                    â”‚
â”‚  (disco)     (texto)     (vetores)      (armazena vetores +       â”‚
â”‚                                          metadados)               â”‚
â”‚                                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         CONSULTA (ask)                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                    â”‚
â”‚  Pergunta â”€â”€â–¶ Embedding â”€â”€â–¶ Qdrant â”€â”€â–¶ Metadados                   â”‚
â”‚  (texto)      (vetor)       (busca)    (paths + linhas)           â”‚
â”‚                                              â”‚                     â”‚
â”‚                                              â–¼                     â”‚
â”‚                              Filesystem â”€â”€â–¶ CÃ³digo Real            â”‚
â”‚                              (leitura)      (contexto)             â”‚
â”‚                                              â”‚                     â”‚
â”‚                                              â–¼                     â”‚
â”‚                              LLM â”€â”€â–¶ Resposta                      â”‚
â”‚                              (geraÃ§Ã£o)                             â”‚
â”‚                                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Por que RAG Ã© Melhor que LLM Puro?

| Aspecto | LLM Puro | RAG |
|---------|----------|-----|
| **Conhecimento** | Limitado ao treinamento | Atualizado com cÃ³digo atual |
| **AlucinaÃ§Ãµes** | Pode inventar cÃ³digo | Responde baseado em contexto real |
| **Especificidade** | GenÃ©rico | EspecÃ­fico para seu projeto |
| **CitaÃ§Ãµes** | NÃ£o consegue citar fontes | Indica arquivos e linhas exatas |
| **Privacidade** | CÃ³digo pode vazar no treinamento | CÃ³digo nunca sai do ambiente local |

## LimitaÃ§Ãµes e ConsideraÃ§Ãµes

### 1. Qualidade depende da indexaÃ§Ã£o
- Se o cÃ³digo relevante nÃ£o foi indexado, nÃ£o serÃ¡ encontrado
- Reindexar apÃ³s mudanÃ§as significativas

### 2. Limite de contexto
- LLMs tÃªm limite de tokens (ex: 8K, 32K)
- Muitos chunks podem exceder o limite
- Use `-k` para ajustar quantidade de contexto

### 3. Similaridade â‰  RelevÃ¢ncia
- Alta similaridade vetorial nÃ£o garante relevÃ¢ncia perfeita
- Reformule a pergunta se resultados nÃ£o forem bons

### 4. CÃ³digo atualizado vs indexado
- Se o arquivo mudou apÃ³s indexaÃ§Ã£o, o contexto pode estar desatualizado
- O sistema lÃª do disco, mas o Qdrant pode apontar para linhas antigas

## Exemplo Completo

**Pergunta:**
```bash
python -m indexer ask "qual banco de dados vetorial Ã© usado neste projeto?"
```

**Logs (o que acontece):**
```
[INFO] Pergunta: qual banco de dados vetorial Ã© usado neste projeto?
[INFO] LLM Model: gpt-oss:latest

# 1. Embedding da pergunta
[INFO] HTTP POST http://localhost:11434/api/embed â†’ 200 OK
[INFO] Vetor size: 3584

# 2. Busca no Qdrant
[INFO] HTTP POST http://localhost:6333/.../query â†’ 200 OK
[INFO] Chunks encontrados: 5

# 3. Leitura dos arquivos (interno, nÃ£o logado)

# 4. Chamada ao LLM
[INFO] Chamando LLM...
[INFO] HTTP POST http://localhost:11434/api/chat â†’ 200 OK
```

**Resposta:**
```
ðŸ’¬ **Pergunta:** qual banco de dados vetorial Ã© usado neste projeto?

ðŸ¤– **Resposta:**
O banco de dados vetorial usado neste projeto Ã© o **Qdrant**.

ðŸ“š **Fontes consultadas:**
  1. apps/docs/pages/ADRs/ADR-02.md (linhas 1-120) - score: 0.8495
  2. .agents/skills/developer-vector-db/SKILL.md (linhas 1-67) - score: 0.8321

â±ï¸  Tempo: 15.32s | Modelo: gpt-oss:latest
```

## Ver TambÃ©m

- [Comando ask](./commands/ask.md) - Uso do comando RAG
- [Comando search](./commands/search.md) - Busca semÃ¢ntica sem LLM
- [Comando index](./commands/index.md) - IndexaÃ§Ã£o de cÃ³digo

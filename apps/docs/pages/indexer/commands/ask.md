
# Code Compass Indexer - Ask Command

O comando `ask` implementa **RAG (Retrieval Augmented Generation)** â€” permite fazer perguntas em linguagem natural sobre o cÃ³digo indexado e receber respostas geradas por um LLM local.

## VisÃ£o Geral

O comando executa as seguintes etapas:
1. Gera embedding da pergunta usando Ollama
2. Busca chunks relevantes no Qdrant (como o `search`)
3. LÃª o conteÃºdo dos arquivos encontrados
4. Monta um prompt com o contexto
5. Envia para um LLM local (Ollama) gerar a resposta

## Uso BÃ¡sico

```bash
python -m indexer ask "sua pergunta aqui"
```

### OpÃ§Ãµes ConfigurÃ¡veis

| OpÃ§Ã£o | Default | DescriÃ§Ã£o |
|-------|---------|-----------|
| `question` | - | Pergunta em linguagem natural (obrigatÃ³rio) |
| `-k`, `--top-k` | `5` | NÃºmero de chunks de contexto |
| `--model` | `gpt-oss:latest` | Modelo LLM para resposta |
| `--ext` | - | Filtrar contexto por extensÃ£o (ex: `.py`) |
| `--min-score` | `0.6` | Score mÃ­nimo de similaridade para usar chunk no contexto |
| `--show-context` | `false` | Mostrar fontes consultadas |
| `--json` | `false` | Output em formato JSON |

### VariÃ¡veis de Ambiente

**LLM:**
| VariÃ¡vel | Default | DescriÃ§Ã£o |
|----------|---------|-----------|
| `LLM_MODEL` | `gpt-oss:latest` | Modelo LLM para gerar respostas |

**Embeddings (Ollama):**
| VariÃ¡vel | Default | DescriÃ§Ã£o |
|----------|---------|-----------|
| `OLLAMA_URL` | `http://localhost:11434` | URL do servidor Ollama |
| `EMBEDDING_MODEL` | `manutic/nomic-embed-code` | Modelo de embedding |

**Qdrant:**
| VariÃ¡vel | Default | DescriÃ§Ã£o |
|----------|---------|-----------|
| `QDRANT_URL` | `http://localhost:6333` | URL do servidor Qdrant |
| `QDRANT_COLLECTION_BASE` | `compass` | Base para nome da collection |

## Exemplos de Uso

### Pergunta BÃ¡sica

```bash
python -m indexer ask "qual banco de dados vetorial Ã© usado neste projeto?"
```

**SaÃ­da:**
```
ğŸ’¬ **Pergunta:** qual banco de dados vetorial Ã© usado neste projeto?

ğŸ¤– **Resposta:**
O banco de dados vetorial usado neste projeto Ã© o **Qdrant**.

â±ï¸  Tempo: 15.32s | Modelo: gpt-oss:latest
```

### Com Modelo EspecÃ­fico

```bash
python -m indexer ask "como funciona o chunking?" --model deepseek-r1:32b
```

### Mostrar Fontes Consultadas

```bash
python -m indexer ask "qual a estrutura do projeto?" --show-context
```

**SaÃ­da:**
```
ğŸ’¬ **Pergunta:** qual a estrutura do projeto?

ğŸ¤– **Resposta:**
O projeto Code Compass Ã© organizado em...

ğŸ“š **Fontes consultadas:**
  1. apps/docs/pages/ADRs/ADR-02.md (linhas 1-120) - score: 0.8495
  2. .agents/skills/architect/SKILL.md (linhas 1-67) - score: 0.8321
  ...

â±ï¸  Tempo: 22.15s | Modelo: gpt-oss:latest
```

### Filtrar por ExtensÃ£o

```bash
python -m indexer ask "como fazer embeddings?" --ext .py
```

### Mais Contexto

```bash
python -m indexer ask "explique a arquitetura completa" -k 10
```

### Output em JSON

```bash
python -m indexer ask "qual o propÃ³sito do indexer?" --json
```

**SaÃ­da JSON:**
```json
{
  "question": "qual o propÃ³sito do indexer?",
  "answer": "O indexer Ã© responsÃ¡vel por...",
  "model": "gpt-oss:latest",
  "contexts_used": 5,
  "elapsed_sec": 18.45,
  "sources": [
    {
      "path": "apps/indexer/README.md",
      "lines": "1-50",
      "score": 0.8654
    }
  ]
}
```

## Detalhes de ImplementaÃ§Ã£o

### Fluxo RAG Completo

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Pergunta  â”‚â”€â”€â”€â”€>â”‚  Embedding  â”‚â”€â”€â”€â”€>â”‚   Qdrant    â”‚
â”‚  (usuÃ¡rio)  â”‚     â”‚  (Ollama)   â”‚     â”‚   (busca)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                                              v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Resposta  â”‚<â”€â”€â”€â”€â”‚     LLM     â”‚<â”€â”€â”€â”€â”‚   Contexto  â”‚
â”‚   (texto)   â”‚     â”‚  (Ollama)   â”‚     â”‚  (chunks)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### System Prompt

O LLM recebe o seguinte prompt de sistema:
```
VocÃª Ã© um assistente especializado em analisar cÃ³digo-fonte.
Responda Ã s perguntas do usuÃ¡rio baseando-se APENAS no contexto fornecido.
Se a informaÃ§Ã£o nÃ£o estiver no contexto, diga que nÃ£o encontrou essa informaÃ§Ã£o no cÃ³digo indexado.
Seja conciso e direto. Responda em portuguÃªs brasileiro.
```

### Leitura de Contexto

Os chunks encontrados sÃ£o lidos diretamente do sistema de arquivos usando:
- `repo_root` do payload (onde o repositÃ³rio foi indexado)
- `path` relativo do arquivo
- `start_line` e `end_line` para extrair apenas o trecho relevante

### Modelos LLM Suportados

Qualquer modelo instalado no Ollama pode ser usado:

```bash
# Listar modelos disponÃ­veis
ollama list

# Exemplos de uso
python -m indexer ask "pergunta" --model gpt-oss:latest
python -m indexer ask "pergunta" --model deepseek-r1:32b
python -m indexer ask "pergunta" --model qwen3-coder:30b
```

## Performance

### Fatores que Afetam o Tempo

| Fator | Impacto |
|-------|---------|
| Tamanho do modelo LLM | Modelos maiores = mais lento |
| NÃºmero de chunks (`-k`) | Mais contexto = prompt maior |
| Complexidade da pergunta | Respostas longas = mais tempo |
| Hardware (GPU/CPU) | GPU acelera significativamente |

### RecomendaÃ§Ãµes

- Para respostas rÃ¡pidas: use modelos menores (`gpt-oss:latest`, `qwen:7b`)
- Para respostas melhores: use modelos maiores (`deepseek-r1:32b`, `qwen3-coder:30b`)
- Para cÃ³digo: use modelos especializados (`qwen3-coder`, `deepseek-coder`)

## Comportamento de Erro

| CenÃ¡rio | Comportamento |
|---------|---------------|
| Ollama indisponÃ­vel | Exit code `1`, mensagem de erro |
| Modelo LLM nÃ£o encontrado | Exit code `1`, mensagem de erro |
| Qdrant indisponÃ­vel | Exit code `1`, mensagem de erro |
| Nenhum contexto encontrado | Mensagem informativa, exit code `0` |
| Arquivo fonte nÃ£o existe mais | Usa placeholder `[arquivo nÃ£o encontrado]` |

## Troubleshooting

### "Erro no embedder/LLM: Erro HTTP 404"
O modelo LLM especificado nÃ£o estÃ¡ instalado.

```bash
# Verificar modelos instalados
ollama list

# Instalar modelo
ollama pull gpt-oss:latest
```

### Resposta muito genÃ©rica ou "nÃ£o encontrei"
- Verifique se o repositÃ³rio foi indexado: `python -m indexer index`
- Aumente o nÃºmero de chunks: `-k 10`
- Reformule a pergunta de forma mais especÃ­fica

### Timeout na resposta
Modelos grandes podem demorar. Alternativas:
- Use um modelo menor: `--model gpt-oss:latest`
- Reduza o contexto: `-k 3`

### Contexto nÃ£o relevante
- Use filtros: `--ext .py` para focar em cÃ³digo Python
- Reindexe com filtros mais especÃ­ficos no `index`
- Aumente `--min-score` (ex: `0.75`) para reduzir chunks fracos
- Garanta que sua indexaÃ§Ã£o ignore ambientes/cache (`.venv`, `venv`, `__pycache__`)

## ComparaÃ§Ã£o: search vs ask

| Aspecto | `search` | `ask` |
|---------|----------|-------|
| **SaÃ­da** | Lista de chunks com scores | Resposta em linguagem natural |
| **Uso do LLM** | NÃ£o | Sim |
| **Tempo** | ~1-2s | ~10-60s (depende do modelo) |
| **Quando usar** | Encontrar arquivos/trechos | Entender o cÃ³digo |

## Ver TambÃ©m

- [Arquitetura RAG](../architecture-rag.md) - Como funciona o RAG internamente
- [search](./search.md) - Busca semÃ¢ntica (sem LLM)
- [init](./init.md) - Inicializa a collection no Qdrant
- [index](./index.md) - Indexa o repositÃ³rio
- [chunk](./chunk.md) - Divide arquivos em chunks
